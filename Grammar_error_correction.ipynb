{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31855cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge_score in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from rouge_score) (3.7)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from rouge_score) (1.23.5)\n",
      "Requirement already satisfied: absl-py in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: click in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: joblib in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (1.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from nltk->rouge_score) (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from click->nltk->rouge_score) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12392604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (2024.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c59580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (0.2.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e58eb3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: requests in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8134f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (0.16.6)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (4.23.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (65.6.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (1.45.0)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\kanishk vijay a t\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdff19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import datasets\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration, T5Tokenizer, \n",
    "    Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "  )\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981bdcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87de9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6071968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('c4_200m_550k.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3a1ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index</td>\n",
       "      <td>The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.</td>\n",
       "      <td>When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.</td>\n",
       "      <td>One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answers Â» Regions Â» Is in Nagorno-Karabakt region that part in Armenia?</td>\n",
       "      <td>Answers Â» Regions Â» Is Nagorno-Karabakh region part of Armenia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flaneuring in fun at maple creek SK!</td>\n",
       "      <td>Flaneuring Fun in Maple Creek SK!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                   input  \\\n",
       "0    The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index   \n",
       "1                                              When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.   \n",
       "2  One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.   \n",
       "3                                                                Answers Â» Regions Â» Is in Nagorno-Karabakt region that part in Armenia?   \n",
       "4                                                                                                   Flaneuring in fun at maple creek SK!   \n",
       "\n",
       "                                                                                                                                   output  \n",
       "0  The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.  \n",
       "1                                                  When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.  \n",
       "2         One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.  \n",
       "3                                                                         Answers Â» Regions Â» Is Nagorno-Karabakh region part of Armenia?  \n",
       "4                                                                                                       Flaneuring Fun in Maple Creek SK!  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d6b090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanishk vijay A T\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e04f6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54c4da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((495000, 2), (55000, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, shuffle=True)\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed00ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "test_df['input_token_len'] = test_df['input'].apply(calc_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5559f0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>input_token_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>486496</th>\n",
       "      <td>You are My Fantasy. And My Reality. So good. ...Sated.</td>\n",
       "      <td>You are My Fantasy. And, My Reality. So good. ...Sated.</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58208</th>\n",
       "      <td>I have always found it be good with.</td>\n",
       "      <td>I have always found it to be good.</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265247</th>\n",
       "      <td>The average rock you would pick up has an SG of about 2.75 because all the earthâ€™s bulk crust is made up of quartz, calcite &amp; feldspar.</td>\n",
       "      <td>The average rock you would pick up has an SG of about 2.75. Because most of the earthâ€™s crust is made up of quartz, calcite &amp; feldspar.</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499297</th>\n",
       "      <td>Bronzes, Mirrors and paintings, fine some art of Sydney.</td>\n",
       "      <td>Bronzes, Mirrors and Paintings, some of the finest in Sydney.</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124684</th>\n",
       "      <td>Is this how america became under Donald Trump?</td>\n",
       "      <td>Is this what America has become under Donald Trump?</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                          input  \\\n",
       "486496                                                                                   You are My Fantasy. And My Reality. So good. ...Sated.   \n",
       "58208                                                                                                      I have always found it be good with.   \n",
       "265247  The average rock you would pick up has an SG of about 2.75 because all the earthâ€™s bulk crust is made up of quartz, calcite & feldspar.   \n",
       "499297                                                                                 Bronzes, Mirrors and paintings, fine some art of Sydney.   \n",
       "124684                                                                                           Is this how america became under Donald Trump?   \n",
       "\n",
       "                                                                                                                                         output  \\\n",
       "486496                                                                                  You are My Fantasy. And, My Reality. So good. ...Sated.   \n",
       "58208                                                                                                        I have always found it to be good.   \n",
       "265247  The average rock you would pick up has an SG of about 2.75. Because most of the earthâ€™s crust is made up of quartz, calcite & feldspar.   \n",
       "499297                                                                            Bronzes, Mirrors and Paintings, some of the finest in Sydney.   \n",
       "124684                                                                                      Is this what America has become under Donald Trump?   \n",
       "\n",
       "        input_token_len  \n",
       "486496               18  \n",
       "58208                10  \n",
       "265247               40  \n",
       "499297               15  \n",
       "124684               12  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "964b1f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1e670ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GrammarDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer,print_text=False):         \n",
    "        self.dataset = dataset\n",
    "        self.pad_to_max_length = False\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_text = print_text\n",
    "        self.max_len = 64\n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    def tokenize_data(self, example):\n",
    "        input_, target_ = example['input'], example['output']\n",
    "        tokenized_inputs = tokenizer(input_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "    \n",
    "        tokenized_targets = tokenizer(target_, pad_to_max_length=self.pad_to_max_length, \n",
    "                                            max_length=self.max_len,\n",
    "                                            return_attention_mask=True)\n",
    "\n",
    "        inputs={\"input_ids\": tokenized_inputs['input_ids'],\n",
    "            \"attention_mask\": tokenized_inputs['attention_mask'],\n",
    "            \"labels\": tokenized_targets['input_ids']\n",
    "        }\n",
    "        \n",
    "        return inputs  \n",
    "    def __getitem__(self, index):\n",
    "        inputs = self.tokenize_data(self.dataset[index])\n",
    "        \n",
    "        if self.print_text:\n",
    "            for k in inputs.keys():\n",
    "                print(k, len(inputs[k]))\n",
    "\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21f2b273",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f494284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanishk vijay A T\\AppData\\Local\\Temp\\ipykernel_14740\\2048908469.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge_metric = load_metric(\"rouge\")\n",
      "C:\\Users\\Kanishk vijay A T\\anaconda3\\lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/rouge/rouge.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "rouge_metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a20676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding='longest', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c58241",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(output_dir=\"weights\",\n",
    "                        evaluation_strategy=\"steps\",\n",
    "                        per_device_train_batch_size=batch_size,\n",
    "                        per_device_eval_batch_size=batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=1,\n",
    "                        weight_decay=0.01,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10f45354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Kanishk vijay A\n",
      "[nltk_data]     T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0db0cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'deep-learning-analytics/GrammarCorrector'\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(torch_device)\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5299fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['They are coming.', \"They're coming.\"]\n"
     ]
    }
   ],
   "source": [
    "text = 'They is coming.'\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
